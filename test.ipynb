{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, auc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ•º lets gooooooooooooooooooooooooooo\n"
     ]
    }
   ],
   "source": [
    "print('ðŸ•º lets gooooooooooooooooooooooooooo' if torch.cuda.is_available() else 'ðŸ’© f@*# the cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Length(nn.Module):\n",
    "    def forward(self, inputs):\n",
    "        return torch.sqrt((inputs ** 2).sum(dim=-1))\n",
    "\n",
    "class Mask(nn.Module):\n",
    "    def forward(self, inputs):\n",
    "        if isinstance(inputs, list):\n",
    "            inputs, mask = inputs\n",
    "        else:\n",
    "            x = torch.sqrt((inputs ** 2).sum(dim=-1))\n",
    "            mask = torch.nn.functional.one_hot(x.argmax(dim=1), num_classes=x.size(1))\n",
    "        inputs_masked = torch.einsum('bij,bj->bi', inputs, mask)\n",
    "        return inputs_masked\n",
    "\n",
    "class CapsuleLayer(nn.Module):\n",
    "    def __init__(self, num_capsule=16, dim_vector=64, num_routing=3):\n",
    "        super(CapsuleLayer, self).__init__()\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_vector = dim_vector\n",
    "        self.num_routing = num_routing\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        u_hat = x.view(batch_size, -1, 1, self.dim_vector)\n",
    "        b_ij = torch.zeros(1, u_hat.size(1), self.num_capsule, 1)\n",
    "        for i in range(self.num_routing):\n",
    "            c_ij = torch.nn.functional.softmax(b_ij, dim=2)\n",
    "            s_j = (c_ij * u_hat).sum(dim=1, keepdim=True)\n",
    "            v_j = squash(s_j)\n",
    "            if i < self.num_routing - 1:\n",
    "                b_ij = b_ij + (u_hat * v_j).sum(dim=-1, keepdim=True)\n",
    "        return v_j.squeeze()\n",
    "\n",
    "def squash(tensor, dim=-1):\n",
    "    squared_norm = (tensor ** 2).sum(dim=dim, keepdim=True)\n",
    "    scale = squared_norm / (1 + squared_norm)\n",
    "    return scale * tensor / torch.sqrt(squared_norm + 1e-9)\n",
    "\n",
    "class PyramidBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(PyramidBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.ln1 = nn.LayerNorm([out_channels, 128, 128]) \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.ln2 = nn.LayerNorm([out_channels, 64, 64]) \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x, residual):\n",
    "        x = torch.cat([x, residual], dim=1)\n",
    "        x = torch.relu(self.ln1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        residual = x\n",
    "        x = torch.relu(self.ln2(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(x)\n",
    "        return x, residual\n",
    "\n",
    "class LaplacianNet(nn.Module):\n",
    "    def __init__(self, input_shape, num_classes, num_routing=3):\n",
    "        super(LaplacianNet, self).__init__()\n",
    "        self.pyramid_block1 = PyramidBlock(input_shape[0] * 2, 64)\n",
    "        self.pyramid_block2 = PyramidBlock(64 + input_shape[0] * 2, 128)\n",
    "        self.pyramid_block3 = PyramidBlock(128 + 64 + input_shape[0] * 2, 256)\n",
    "        \n",
    "        self.primary_caps = nn.Conv2d(256, 32 * 8, kernel_size=11, stride=1, padding=0)\n",
    "        self.primary_caps_reshape = nn.Sequential(\n",
    "            nn.Conv2d(32 * 8, 32 * 8, kernel_size=1),\n",
    "            nn.BatchNorm2d(32 * 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 8 * 2 * 2, 32 * 8)\n",
    "        )\n",
    "        self.capsule_layer = CapsuleLayer(num_capsule=num_classes, dim_vector=8, num_routing=num_routing)\n",
    "        self.length = Length()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(8 * num_classes, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, np.prod(input_shape)),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Unflatten(1, input_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, y=None):\n",
    "        x_g1, x_g2, x_g3, x_g4, x_l1, x_l2, x_l3, x_l4 = inputs\n",
    "        \n",
    "        x1, r1 = self.pyramid_block1(x_g1, x_l1)\n",
    "        x2, r2 = self.pyramid_block2(x1, torch.cat([x_g2, x_l2], dim=1))\n",
    "        x3, _ = self.pyramid_block3(x2, torch.cat([x_g3, x_l3], dim=1))\n",
    "        x = torch.cat([x3, x_g4, x_l4], dim=1)\n",
    "        \n",
    "        x = self.primary_caps(x)\n",
    "        x = self.primary_caps_reshape(x)\n",
    "        x = self.capsule_layer(x)\n",
    "        \n",
    "        if y is not None:\n",
    "            masked = Mask()([x, y])\n",
    "        else:\n",
    "            masked = Mask()(x)\n",
    "            \n",
    "        decoded = self.decoder(masked)\n",
    "        length = self.length(x)\n",
    "        return length, decoded\n",
    "\n",
    "def margin_loss(y_true, y_pred):\n",
    "    alpha_margin = torch.clamp(0.9 - y_pred, min=0.0) ** 2\n",
    "    beta_margin = torch.clamp(y_pred - 0.1, min=0.0) ** 2\n",
    "    L = y_true * alpha_margin + 0.5 * (1 - y_true) * beta_margin\n",
    "    return L.mean()\n",
    "\n",
    "class ImagePyramid:\n",
    "    def __init__(self, data, **kw):\n",
    "        self.data = data\n",
    "        self.name = kw.get(\"name\", \"imgpyr\")\n",
    "        self.verbose = kw.get(\"verbose\", True)\n",
    "        self.num_classes = kw.get(\"num_classes\", 16)\n",
    "        self.input_shape = kw.get(\"input_shape\", (1, 128, 128))\n",
    "        self.batch_size = kw.get(\"batch_size\", 32)\n",
    "        self.recon_loss = kw.get(\"recon_loss\", nn.L1Loss())\n",
    "        self.watch = kw.get(\"monitor\", \"val_loss\")\n",
    "        self.wait = kw.get(\"patience\", 10)\n",
    "        self.path = kw.get(\"path\", f\"weights/{self.name}_weights.pth\")\n",
    "        self.__get_model__()\n",
    "\n",
    "    def __get_model__(self):\n",
    "        self.model = LaplacianNet(self.input_shape, self.num_classes)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-4, amsgrad=True)\n",
    "        self.criterion = margin_loss\n",
    "        self.__setup_callbacks__()\n",
    "\n",
    "    def __setup_callbacks__(self):\n",
    "        self.early_stop = None  # Implement early stopping if needed\n",
    "        self.checkpoint = self.path\n",
    "        self.reduce_on_plateau = None  # Implement LR reduction if needed\n",
    "\n",
    "    def fit(self):\n",
    "        dataloader = DataLoader(self.data, batch_size=self.batch_size, shuffle=True)\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(1000):\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            for data in dataloader:\n",
    "                inputs, y = data\n",
    "                self.optimizer.zero_grad()\n",
    "                output, reconstructed = self.model(inputs, y)\n",
    "                loss = self.criterion(y, output) + self.recon_loss(reconstructed, inputs[0])\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            train_loss /= len(dataloader)\n",
    "            print(f'Epoch {epoch}, Loss: {train_loss}')\n",
    "\n",
    "            if train_loss < best_loss:\n",
    "                best_loss = train_loss\n",
    "                torch.save(self.model.state_dict(), self.checkpoint)\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter > self.wait:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        test_dataloader = DataLoader(test_data, batch_size=self.batch_size, shuffle=False)\n",
    "        self.model.load_state_dict(torch.load(self.checkpoint))\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in test_dataloader:\n",
    "                inputs, y = data\n",
    "                output, _ = self.model(inputs)\n",
    "                pred = output.argmax(dim=1)\n",
    "                correct += (pred == y).sum().item()\n",
    "                total += y.size(0)\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Accuracy: {accuracy}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "    def upscale(self, img, target_size):\n",
    "        return transforms.functional.resize(img, target_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "\n",
    "        # Gaussian pyramid levels as PIL Images\n",
    "        g1 = transforms.ToPILImage()(img)\n",
    "        g2 = transforms.Resize((64, 64))(g1)\n",
    "        g3 = transforms.Resize((32, 32))(g1)\n",
    "        g4 = transforms.Resize((16, 16))(g1)\n",
    "\n",
    "        # Convert back to tensors\n",
    "        g1 = self.to_tensor(g1)\n",
    "        g2 = self.to_tensor(g2)\n",
    "        g3 = self.to_tensor(g3)\n",
    "        g4 = self.to_tensor(g4)\n",
    "\n",
    "        # Laplacian pyramid differences\n",
    "        l1 = g1\n",
    "        l2 = transforms.Resize((64, 64))(g1 - self.upscale(g2, g1.shape[-2:]))\n",
    "        l3 = transforms.Resize((32, 32))(g2 - self.upscale(g3, g2.shape[-2:]))\n",
    "        l4 = transforms.Resize((16, 16))(g3 - self.upscale(g4, g3.shape[-2:]))\n",
    "\n",
    "        return [g1, g2, g3, g4, l1, l2, l3, l4], label\n",
    "\n",
    "def plot_curves(train_loss, val_loss, dataset_name):\n",
    "    plt.figure()\n",
    "    plt.plot(train_loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title(f'Training and Validation Loss Curves for {dataset_name}')\n",
    "    plt.savefig(f'results/{dataset_name}_loss_curve.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(cm, dataset_name, phase):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix for {dataset_name} - {phase}')\n",
    "    plt.savefig(f'results/{dataset_name}_confusion_matrix_{phase}.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc_auc(y_true, y_score, dataset_name):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'Receiver Operating Characteristic for {dataset_name}')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig(f'results/{dataset_name}_roc_auc.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: ../data/train_32x32.mat\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('results', exist_ok=True)\n",
    "os.makedirs('weights', exist_ok=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "datasets = {\n",
    "    'CIFAR10': torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=transform),\n",
    "    'CIFAR100': torchvision.datasets.CIFAR100(root='../data', train=True, download=True, transform=transform),\n",
    "    'MNIST': torchvision.datasets.MNIST(root='../data', train=True, download=True, transform=transform),\n",
    "    'FashionMNIST': torchvision.datasets.FashionMNIST(root='../data', train=True, download=True, transform=transform),\n",
    "    'SVHN': torchvision.datasets.SVHN(root='../data', split='train', download=True, transform=transform),\n",
    "    'STL10': torchvision.datasets.STL10(root='../data', split='train', download=True, transform=transform),\n",
    "    # 'ImageNet': torchvision.datasets.ImageNet(root='../data', split='train', download=True, transform=transform),\n",
    "    'Caltech101': torchvision.datasets.Caltech101(root='../data', download=True, transform=transform),\n",
    "    'Caltech256': torchvision.datasets.Caltech256(root='../data', download=True, transform=transform),\n",
    "    'CelebA': torchvision.datasets.CelebA(root='../data', split='train', download=True, transform=transform),\n",
    "    # 'LSUN': torchvision.datasets.LSUN(root='../data', classes='train', transform=transform,),\n",
    "    'Omniglot': torchvision.datasets.Omniglot(root='../data', download=True, transform=transform),\n",
    "    'OxfordIIITPet': torchvision.datasets.OxfordIIITPet(root='../data', split='trainval', download=True, transform=transform),\n",
    "    # 'StanfordCars': torchvision.datasets.StanfordCars(root='../data', split='train', download=True, transform=transform),\n",
    "    'SBD': torchvision.datasets.SBDataset(root='../data', image_set='train', download=False)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(dataset_name, train_data, test_data, device):\n",
    "    train_loader = DataLoader(ImageDataset(train_data), batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(ImageDataset(test_data), batch_size=32, shuffle=False)\n",
    "\n",
    "    num_classes = len(train_data.classes)\n",
    "    input_shape = train_data[0][0].shape\n",
    "    print(input_shape)\n",
    "    model = LaplacianNet(input_shape, num_classes).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4, amsgrad=True)\n",
    "    criterion = margin_loss\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = [inp.to(device) for inp in inputs]\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs, reconstructed = model(inputs, labels)\n",
    "            loss = criterion(labels, outputs) + nn.L1Loss()(reconstructed, inputs[0])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        y_score = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs = [inp.to(device) for inp in inputs]\n",
    "                labels = labels.to(device)\n",
    "                outputs, _ = model(inputs)\n",
    "                loss = criterion(labels, outputs)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "                y_pred.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "                y_score.extend(outputs.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(test_loader)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save(model.state_dict(), f'weights/{dataset_name}_best.pth')\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter > 10:  # patience\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{100}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    plot_curves(train_losses, val_losses, dataset_name)\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_score = np.array(y_score)\n",
    "\n",
    "    cm_train = confusion_matrix(y_true, y_pred)\n",
    "    plot_confusion_matrix(cm_train, dataset_name, 'train')\n",
    "\n",
    "    roc_auc = roc_auc_score(y_true, y_score, multi_class='ovr')\n",
    "    plot_roc_auc(y_true, y_score, dataset_name)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on CIFAR10 dataset\n",
      "Files already downloaded and verified\n",
      "torch.Size([3, 128, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 64 but got size 128 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m train_data \u001b[38;5;241m=\u001b[39m dataset\n\u001b[1;32m     10\u001b[0m test_data \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[name](root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 25\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(dataset_name, train_data, test_data, device)\u001b[0m\n\u001b[1;32m     22\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 25\u001b[0m outputs, reconstructed \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(labels, outputs) \u001b[38;5;241m+\u001b[39m nn\u001b[38;5;241m.\u001b[39mL1Loss()(reconstructed, inputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     27\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/torch-rl/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch-rl/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 88\u001b[0m, in \u001b[0;36mLaplacianNet.forward\u001b[0;34m(self, inputs, y)\u001b[0m\n\u001b[1;32m     85\u001b[0m x_g1, x_g2, x_g3, x_g4, x_l1, x_l2, x_l3, x_l4 \u001b[38;5;241m=\u001b[39m inputs\n\u001b[1;32m     87\u001b[0m x1, r1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpyramid_block1(x_g1, x_l1)\n\u001b[0;32m---> 88\u001b[0m x2, r2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpyramid_block2(x1, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_g2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_l2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     89\u001b[0m x3, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpyramid_block3(x2, torch\u001b[38;5;241m.\u001b[39mcat([x_g3, x_l3], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     90\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x3, x_g4, x_l4], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 64 but got size 128 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "for name, dataset in datasets.items():\n",
    "    print(f\"Training on {name} dataset\")\n",
    "    train_data = dataset\n",
    "    test_data = torchvision.datasets.__dict__[name](root='./data', train=False, download=True, transform=transform)\n",
    "    model = train_and_evaluate(name, train_data, test_data, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
